# Object Detection

This sample is a complete example that shows you how to perform **Object detection** in a ZED Hub App. It also shows how to generate:
- **Logs** that inform you about the app's status
- **Telemetries** which store data linked to the detections
- **Video Events** that capture videos when people are detected
- A **Custom Stream** which shows you the live video, with bounding boxes around detected people

The app defines **parameters** that can be modified in the ZED Hub interface to change the app's behavior while it is running.

![](./icon.png " ")


## Getting started

- Add and set up your device on [ZED Hub](https://hub.stereolabs.com).
- For more information, read the [requirements](../../README.md#requirements).


## Build and deploy this tutorial

### How to build your application (for development)

Build your app :
```bash
$ cd sources
$ mkdir build
$ cd build
$ cmake ..
$ make -j$(nproc)
```

This application defines application parameters to modify its behavior while it is running. Move the `parameters.json` file to the path you specified in the `HubClient::loadApplicationParameters` function.
```bash
$ cp ../parameters.json .
```

Then to run your app :
```bash
$ ./Object_Detection_Sample
```

To dynamically change the parameters and activate their callbacks, edit the `parameters.json` file.

### How to deploy your application

Packages your app by generating an app.zip file using :

```bash
$ edge_cli deploy .
```

You can now [deploy your app](https://www.stereolabs.com/docs/cloud/applications/deployment/) using the ZED Hub interface:

- In your workspace, in the **Applications** section, click on **Add application**
- Select the ZIP file containing the application in your filesystem
- Select the devices on which you want to deploy the app and press **Upload**

## What you should see after deployment
### Live video

A video with bounding boxes around detected persons should be available in the **Video** panel

### Logs
The logs should inform you about the app status.


### Telemetry
The telemetries containing data about detection should be available and generated every 10 seconds by default.

### Video Event

A video is considered an event if **at least one person is detected** in the image. Therefore if your app is running and someone is seen by your ZED, you should see an **Event** in the **Video Event panel** corresponding to this situation.

You can click on it. You have access to the video and the stored data of the event. You have access to a longer video than the exact event duration ( you can watch a few seconds before and after the event). The blue line indicates which part of the video is associated with the event.

### Parameters

Five parameters can be used to modify your app behavior:

- **Draw Bboxes**: If selected, will draw boxes around detected people in the camera live view.

- **Enable Video Events**: If selected some **Video Event** will be recorded when someone is detected on the video.

- **Number of frames without det Btw 2 Events** (number of frames without detection between two video events): If no one is seen for the defined number of frames, the next time that person is detected a new event is defined instead of continuing the previous event.


- **Record Telemetry**: If selected, the app will send the number of people detected and other info as telemetry.

- **Telemetry frequency**: Telemetry frequency defines how often telemetries are generated by the app. Every 10 seconds by default.


## Code overview


### Parameters callback
Some callbacks are defined and will be called when a parameter will be modified through the interface. They are used to modify the parameter value and notify that the change has been done.

Callback for `draw_bboxes` parameter
```c++
void onDisplayParametersUpdate(FunctionEvent &event)
{
    event.status = 0;
    draw_bboxes = HubClient::getParameter<bool>("draw_bboxes", PARAMETER_TYPE::APPLICATION, draw_bboxes);
    HubClient::sendLog("New parameter : draw_bboxes modified", LOG_LEVEL::INFO);
}
```


### Initialization
The application connects to ZED Hub with `HubClient::connect` and registers the opened ZED camera with `HubClient::registerCamera`.
**Object detection** is enabled with `enableObjectDetection`. 
> **Note**: The tracking is required to use it (`enablePositionalTracking` must be called).

```c++
    auto zed_error =  p_zed->enablePositionalTracking(track_params);
    ...
    zed_error = p_zed->enableObjectDetection(obj_det_params);
```

The detection is limited to `PERSON` (meaning that the Vehicles for instance are ignored), and the detection threshold is set to 50:

```c++
    // Object Detection runtime parameters : detect person only
    objectTracker_parameters_rt.detection_confidence_threshold = 50;
    objectTracker_parameters_rt.object_class_filter.clear();
    objectTracker_parameters_rt.object_class_filter.push_back(sl::OBJECT_CLASS::PERSON);
```

The parameters are associated with the callbacks that have been defined above. Here the callback `onDisplayParametersUpdate`is associated to `draw_bboxes`. It will be called when `draw_bboxes` will be modified.

```c++
    CallbackParameters callback_display_param;
    callback_display_param.setParameterCallback("onDisplayParametersUpdate", "draw_bboxes", CALLBACK_TYPE::ON_PARAMETER_UPDATE, PARAMETER_TYPE::APPLICATION);
    HubClient::registerFunction(onDisplayParametersUpdate, callback_display_param);

```


### Main loop

Each time a frame is successfully **grabbed**, the detected objects are retrieved by the `retrieveObjects` function and stored in `objects`.

Then you will find the features described in the tutorials:

- **Video events** are defined exactly in the same way as in [`tutorial_06_video_event`](../../tutorials/tutorial_06_video_event/README.md)

```c++
sl::Timestamp current_ts = objects.timestamp;
if (recordVideoEvent && counter_reliable_objects >= 1)
{
    bool is_new_event = true;
    if (!first_event_sent || counter_no_detection >= nbFramesNoDetBtw2Events)
    {
        event_reference = "detected_person_" + std::to_string(current_ts.getMilliseconds());
        HubClient::sendLog("New Video Event defined", LOG_LEVEL::INFO);
    }
    else
    {
        // Do nothing, keep previous event reference --> The current frame will be defined as being part of the previous video event
        is_new_event = false;
    }

    EventParameters event_params;
    event_params.timestamp = current_ts.getMilliseconds();
    event_params.reference = event_reference;
    std::string event_label = "People Detection"; // or label of your choice
    json event2send;                              // Use to store all the data associated to the video event.
    event2send["message"] = "Current event as reference " + event_reference;
    event2send["nb_detected_person"] = objects.object_list.size();

    if (is_new_event || !first_event_sent)
    {
        HubClient::startVideoEvent(p_zed, event_label, event2send, event_params);
        first_event_sent = true;
    }
    // update every 10 s
    else if ((uint64)(current_ts.getMilliseconds() >= (uint64)(prev_timestamp.getMilliseconds() + (uint64)10 * 1000ULL)))
    {
        HubClient::updateVideoEvent(p_zed, event_label, event2send, event_params);
    }
    // else do nothing

    counter_no_detection = 0; // reset counter as someone as been detected
}
else
{
    counter_no_detection++;
}
```


- A **custom stream** is sent using the RGB image and OpenCV. To send a custom stream, just add the `sl::Mat` as an argument of `update()`.

```c++
if (draw_bboxes)
{
    p_zed->retrieveImage(imgLeftCustom, sl::VIEW::LEFT, sl::MEM::CPU, imgLeftCustom.getResolution());

    float ratio_x = (float)leftImageCpuCV.cols / (float)image_raw_res.width;
    float ratio_y = (float)leftImageCpuCV.rows / (float)image_raw_res.height;

    for (int i = 0; i < objects.object_list.size(); i++)
    {
        if (objects.object_list[i].tracking_state == sl::OBJECT_TRACKING_STATE::OK)
        {
            sl::uint2 tl = objects.object_list[i].bounding_box_2d[0];
            sl::uint2 br = objects.object_list[i].bounding_box_2d[2];
            cv::Rect ROI = cv::Rect(cv::Point2i(tl.x * ratio_x, tl.y * ratio_y), cv::Point2i(br.x * ratio_x, br.y * ratio_y));
            cv::Scalar color = cv::Scalar(50, 200, 50, 255);
            cv::rectangle(leftImageCpuCV, ROI, color, 2);
        }
    }

    HubClient::update(p_zed, imgLeftCustom);
}
else
    // Always update Hub at the end of the grab loop
    HubClient::update(p_zed);
```

- **Telemetry** relative to the detected objects are defined and sent. See [**tutorial_03_telemetries**](../../tutorials/tutorial_03_telemetries/README.md).

```c++
sl::Timestamp curr_timestamp = p_zed->getTimestamp(sl::TIME_REFERENCE::IMAGE);
if (recordTelemetry && (uint64)(current_ts.getMilliseconds() >= (uint64)(prev_timestamp.getMilliseconds() + (uint64)telemetryFreq * 1000ULL)))
{
    float mean_distance = 0;
    // compute objects ( = people)  mean distance from camera. This value will be sent as telemetry
    for (int i = 0; i < objects.object_list.size(); i++)
    {
        mean_distance += (objects.object_list[i].position).norm();
    }

    if (objects.object_list.size() > 0)
    {
        mean_distance /= (float)objects.object_list.size();
    }

    // Send Telemetry
    sl_hub::json position_telemetry;
    position_telemetry["number_of_detection"] = objects.object_list.size();
    position_telemetry["mean_distance_from_cam"] = mean_distance;
    HubClient::sendTelemetry("object_detection", position_telemetry);
    prev_timestamp = current_ts;
}
```

